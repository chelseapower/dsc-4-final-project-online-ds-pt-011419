{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"student.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"eqDoad7P0KUD","colab_type":"text"},"source":["## Module 4 Final Project Submission\n","\n","Please fill out:\n","* Student name: **Chelsea Power**\n","* Student pace: **part time**\n","* Scheduled project review date/time: **8/30/19 at 5 pm ET**\n","* Instructor name: **Brandon Lewis**\n","* Blog post URL: \n"]},{"cell_type":"markdown","metadata":{"id":"DkCtlp2-8IAM","colab_type":"text"},"source":["## Purpose\n","\n","This project will focus on predicting customer mood for the purpose of song recommendation and maintaining listener engagement. A neural network will be used to predict the song mood (emotion variation of a user based on the track they are playing). Then based on the output of the neural network, a conext-aware recommendation system will recommend the next song.\n","\n","## Data Dictionary\n","\n","**# nowplaying-RS Dataset (3 files)**\n","\n","This dataset features context- and content features of listening events. It contains 11.6 million music listening events of 139K users and 346K tracks collected from Twitter. The dataset comes with a rich set of item content features and user context features, as well as timestamps of the listening events. Moreover, some of the user context features imply the cultural origin of the users, and some others - like hashtags - give clues to the emotional state of a user underlying a listening event.\n","\n","- **user_track_hashtag_timestamp.csv** contains basic information about each listening event. Provided in each listening event: id, the user_id, track_id, hashtag, created_at\n","- **context_content_features.csv** contains all context and content features. Provided in each listening event: the id of the event, user_id, track_id, artist_id, content features regarding the track mentioned in the event (instrumentalness, liveness, speechiness, danceability, valence, loudness, tempo, acousticness, energy, mode, key) and context features regarding the listening event (coordinates (as geoJSON), place (as geoJSON), geo (as geoJSON), tweet_language, created_at, user_lang, time_zone, entities contained in the tweet).\n","- **sentiment_values.csv** contains sentiment information for hashtags. It contains the hashtag itself and the sentiment values gathered via four different sentiment dictionaries: AFINN, Opinion Lexicon, Sentistrength Lexicon and vader. For each of these dictionaries we list the minimum, maximum, sum and average of all sentiments of the tokens of the hashtag (if available, else we list empty values). However, as most hashtags only consist of a single token, these values are equal in most cases. Please note that the lexica are rather diverse and therefore, are able to resolve very different terms against a score. Hence, the resulting csv is rather sparse. The file contains the following comma-separated values: <hashtag, vader_min, vader_max, vader_sum,vader_avg,  afinn_min, afinn_max, afinn_sum, afinn_avg, ol_min, ol_max, ol_sum, ol_avg, ss_min, ss_max, ss_sum, ss_avg >, where we abbreviate all scores gathered over the Opinion Lexicon with the prefix 'ol'. Similarly, 'ss' stands for SentiStrength. "]},{"cell_type":"markdown","metadata":{"id":"PZE8buIr_96g","colab_type":"text"},"source":["### OBSERVE: Understand and Load the Datasets"]},{"cell_type":"code","metadata":{"id":"uh2NtS5tS65h","colab_type":"code","colab":{}},"source":["# Import required libraries\n","import pandas as pd\n","import numpy as np\n","\n","df1 = pd.read_csv('user_track_hashtag_timestamp.csv')\n","\n","#Look at size of the dataset\n","df1.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5wNI2-VALG0","colab_type":"code","colab":{}},"source":["#Look at the columns and first 10 rows of the dataset\n","df1.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ts9KG0hWkJL","colab_type":"code","colab":{}},"source":["#Load second dataset\n","df2 = pd.read_csv('context_content_features.csv')\n","\n","#Look at size of the dataset\n","df2.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNNbwZuEWsfh","colab_type":"code","colab":{}},"source":["#Look at the columns and first 10 rows of the dataset\n","df2.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzei-MQPWst8","colab_type":"code","colab":{}},"source":["#Load third dataset\n","df3 = pd.read_csv('sentiment_values.csv.csv')\n","\n","#Look at size of the dataset\n","df3.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7sneJrHWs6N","colab_type":"code","colab":{}},"source":["#Look at the columns and first 10 rows of the dataset\n","df3.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVuJCbuAXHZz","colab_type":"code","colab":{}},"source":["#Determine how to join datasets - rename columns / drop unnecessary columns per csv file\n","#Merge csv files into new dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2b1aPzUASa-","colab_type":"text"},"source":["### SCRUB: Data Preparation\n","- Data type conversions (e.g. numeric data mistakenly encoded as objects)\n","- Detect and deal with missing values\n","- Remove unnecessary columns"]},{"cell_type":"code","metadata":{"id":"vf5ewucNAY-L","colab_type":"code","colab":{}},"source":["#Look at column types"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9ts1pw97iz8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtrJgKGtCHVI","colab_type":"text"},"source":["## Check for null/missing values"]},{"cell_type":"code","metadata":{"id":"3vcYoKQaCLj2","colab_type":"code","colab":{}},"source":["#Run an apply method utilizing a lambda expression that checks to see if there was any missing values through each column. \n","#Printing the column name and total missing values for that column, iteratively.\n","df_music.apply(lambda x: x.isnull().sum())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oGwjoO17iSV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9s71PmNjAZ1u","colab_type":"text"},"source":["## Explore the data\n","- Look at the distribution for the data\n","- Look for multicolinarity\n","- Remove unnecessary features"]},{"cell_type":"code","metadata":{"id":"c8xWj-IgCaur","colab_type":"code","colab":{}},"source":["#Look at value counts of the predictor variable\n","df_music.XXX.value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGiuRx3HChqn","colab_type":"code","colab":{}},"source":["# Visualize data\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.countplot(x='XXX', data=df, palette='hls')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-qyDJdPnCnWe","colab_type":"text"},"source":["**Observation:**"]},{"cell_type":"code","metadata":{"id":"_jCFeh1XCqfk","colab_type":"code","colab":{}},"source":["# Create continuous dataset and look at distributions for data\n","df_music ="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUwPkRWDCz8r","colab_type":"text"},"source":["**Summary:**"]},{"cell_type":"code","metadata":{"id":"8vOM3ZLOC3qK","colab_type":"code","colab":{}},"source":["#Create coorelation heatmap - check for multicolinarity\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","correlation = df_music.corr()\n","plt.figure(figsize=(14, 12))\n","heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQzVGgLkC8Fr","colab_type":"text"},"source":["**Summary:**"]},{"cell_type":"markdown","metadata":{"id":"RV-Hdk96Dfzx","colab_type":"text"},"source":["## Import packages and generate the data"]},{"cell_type":"code","metadata":{"id":"siErHIUc0lhf","colab_type":"code","colab":{}},"source":["# Package imports\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sklearn\n","from sklearn.datasets import make_classification\n","import sklearn.linear_model\n","\n","# Display plots inline and change default figure size\n","%matplotlib inline\n","matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8orU4s1O0uKZ","colab_type":"code","colab":{}},"source":["# Generate a dataset and plot it\n","np.random.seed(123)\n","sample_size = 500\n","X, Y = sklearn.datasets.make_circles(n_samples = sample_size, noise = 0.1)\n","\n","# Display plots inline and change default figure size\n","%matplotlib inline\n","matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n","plt.scatter(X[:,0], X[:,1], s=20, c=Y, edgecolors=\"gray\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LuhZrgEtDIvd","colab_type":"text"},"source":["## Model 1: Logistic Regression\n","- Normalize the data prior to fitting the model\n","- Train-Test Split\n","- Fit the model\n","- Predict\n","- Evaluate"]},{"cell_type":"code","metadata":{"id":"c0jonMA1DHhu","colab_type":"code","colab":{}},"source":["#Train the logistic regression classifier\n","clf = sklearn.linear_model.LogisticRegressionCV()\n","clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tg9SdQgY1GEo","colab_type":"code","colab":{}},"source":["# Helper function to plot a decision boundary that will visualize the classification performance\n","def plot_decision_boundary(pred_func):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n","    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n","    h = 0.01\n","    \n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    \n","    # Predict the function value for the whole gid\n","    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    \n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-AYtDrw1SzV","colab_type":"code","colab":{}},"source":["#Create a decision boundart using the predictions made by the logistic regression model\n","plot_decision_boundary(lambda x: log_reg.predict(x))\n","plt.title(\"Logistic Regression\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLDTWtqd1ej8","colab_type":"code","colab":{}},"source":["#Store the predictions to calculate the accuracy\n","clf_predict = clf.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWcnMKTX1j_Q","colab_type":"code","colab":{}},"source":["print ('The logistic regression model has an accuracy of: ' \n","       + str(np.sum(clf_predict == Y)/sample_size*100) + '%')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o0pznT_PDsu3","colab_type":"text"},"source":["## Decision Tree"]},{"cell_type":"code","metadata":{"id":"slc3Jc_CDu5S","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL_lLzT1DTj_","colab_type":"text"},"source":["## Build a neural network"]},{"cell_type":"code","metadata":{"id":"wj6ZEn00DRg8","colab_type":"code","colab":{}},"source":["#Define some useful variables and parameters for gradient descent:\n","num_examples = len(X) #training set size\n","nn_input_dim = 2 #input layer dimensionality\n","nn_output_dim = 2 #output layer dimensionality\n","\n","# Gradient descent parameters (these may need to change)\n","epsilon = 0.01 # learning rate for gradient descent\n","reg_lambda = 0.01 # regularization strength"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXBqG4BG1y-T","colab_type":"code","colab":{}},"source":["#Helper function to evaluate the total loss on the dataset\n","def calculate_loss(model):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    # Forward propagation to calculate our predictions\n","    z1 = X.dot(W1) + b1\n","    a1 = np.tanh(z1)\n","    z2 = a1.dot(W2) + b2\n","    exp_scores = np.exp(z2)\n","    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","    # Calculating the loss\n","    corect_logprobs = -np.log(probs[range(num_examples), y])\n","    data_loss = np.sum(corect_logprobs)\n","    # Add regulatization term to loss (optional)\n","    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n","    return 1./num_examples * data_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"suil36eh14iE","colab_type":"code","colab":{}},"source":["#Helper function to predict an output (0 or 1)\n","def predict(model, x):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    # Forward propagation\n","    z1 = x.dot(W1) + b1\n","    a1 = np.tanh(z1)\n","    z2 = a1.dot(W2) + b2\n","    exp_scores = np.exp(z2)\n","    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","    return np.argmax(probs, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOO5uJ0r2Aui","colab_type":"code","colab":{}},"source":["# This function learns parameters for the neural network and returns the model.\n","# - nn_hdim: Number of nodes in the hidden layer\n","# - num_passes: Number of passes through the training data for gradient descent\n","# - print_loss: If True, print the loss every 1000 iterations\n","def build_model(nn_hdim, num_passes=20000, print_loss=False):\n","    \n","    # Initialize the parameters to random values. We need to learn these.\n","    np.random.seed(0)\n","    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n","    b1 = np.zeros((1, nn_hdim))\n","    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n","    b2 = np.zeros((1, nn_output_dim))\n","\n","    # This is what we return at the end\n","    model = {}\n","    \n","    # Gradient descent. For each batch...\n","    for i in range(0, num_passes):\n","\n","        # Forward propagation\n","        z1 = X.dot(W1) + b1\n","        a1 = np.tanh(z1)\n","        z2 = a1.dot(W2) + b2\n","        exp_scores = np.exp(z2)\n","        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","\n","        # Backpropagation\n","        delta3 = probs\n","        delta3[range(num_examples), y] -= 1\n","        dW2 = (a1.T).dot(delta3)\n","        db2 = np.sum(delta3, axis=0, keepdims=True)\n","        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n","        dW1 = np.dot(X.T, delta2)\n","        db1 = np.sum(delta2, axis=0)\n","\n","        # Add regularization terms (b1 and b2 don't have regularization terms)\n","        dW2 += reg_lambda * W2\n","        dW1 += reg_lambda * W1\n","\n","        # Gradient descent parameter update\n","        W1 += -epsilon * dW1\n","        b1 += -epsilon * db1\n","        W2 += -epsilon * dW2\n","        b2 += -epsilon * db2\n","        \n","        # Assign new parameters to the model\n","        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","        \n","        # Optionally print the loss.\n","        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n","        if print_loss and i % 1000 == 0:\n","          print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-zWGX35161v6","colab_type":"text"},"source":["## A network with a hidden layer of size 3\n","Train the network with a hidden layer size of 3"]},{"cell_type":"code","metadata":{"id":"F4JhliFz2MAl","colab_type":"code","colab":{}},"source":["# Build a model with a 3-dimensional hidden layer\n","model = build_model(3, print_loss=True)\n","\n","# Plot the decision boundary\n","plot_decision_boundary(lambda x: predict(model, x))\n","plt.title(\"Decision Boundary for hidden layer size 3\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Qs-rDCh7IxC","colab_type":"text"},"source":["## Varying the hidden layer size"]},{"cell_type":"code","metadata":{"id":"jgDacyHqDZX4","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(16, 32))\n","hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n","for i, nn_hdim in enumerate(hidden_layer_dimensions):\n","    plt.subplot(5, 2, i+1)\n","    plt.title('Hidden Layer size %d' % nn_hdim)\n","    model = build_model(nn_hdim)\n","    plot_decision_boundary(lambda x: predict(model, x))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YVuLkHnt_Zlg","colab_type":"text"},"source":["### Citation\n","\n","@inproceedings{smc18,\n","title = {#nowplaying-RS: A New Benchmark Dataset for Building Context-Aware Music Recommender Systems},\n","author = {Asmita Poddar and Eva Zangerle and Yi-Hsuan Yang},\n","url = {http://mac.citi.sinica.edu.tw/~yang/pub/poddar18smc.pdf},\n","year = {2018},\n","date = {2018-07-04},\n","booktitle = {Proceedings of the 15th Sound & Music Computing Conference},\n","address = {Limassol, Cyprus},\n","note = {code at https://github.com/asmitapoddar/nowplaying-RS-Music-Reco-FM},\n","tppubtype = {inproceedings}\n","}"]}]}